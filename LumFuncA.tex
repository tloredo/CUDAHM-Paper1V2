%===============================================================================
\section{Luminosity function estimation: Framework}
\label{sec:lum_func}

The CUDAHM distribution contains some example code implementing basic hierarchical models, such as the normal-normal model, and a realistically complicated astrophysical example handling regression with classical measurement error, with nonlinear models (the interstellar dust problem described above).

As a somewhat more complicated application of CUDAHM, we here consider luminosity function estimation, a parametric conditional density estimation problem arising across many areas of astronomy.
We highlight this example, both because of its ubiquity across astronomy, and to illustrate the generality of CUDAHM.
We describe luminosity function estimation for a model including not only measurement error, but also \emph{selection effects}.
The selection effects make the joint distribution structure more complicated than the conditional density estimation DAG shown in Fig.~\ref{fig:DAGs}; in particular, there are two plates (corresponding to detected and undetected objects).
However, the likelihood function can be manipulated to have a conditional dependence structure similar to that of a single-plate DAG, allowing straightforward implementation of the model using CUDAHM.

%-------------------------------------------------------------------------------
\subsection{Astronomical background}
\label{sec:LF-astro}

The fundamental observables for localized astronomical sources include position (both direction on the celestial sphere, and distance, $r$, in some chosen coordinate system), and apparent brightness, quantified in terms of flux, $F$ (astronomers also often report brightness using magnitudes, a negative logarithmic relative flux scale).
Astronomers use these observables to study demographic properties of many classes of sources, including stars and galaxies of various types, minor planets (such as asteroids), and explosive transients (gamma-ray bursts, supernovae).
For concreteness, here we focus on observations of nearby galaxies, for which distance may be measured by using spectroscopy to find the \emph{redshift}, $z$, of spectral lines (i.e., their fractional shift in wavelength from laboratory values).
Due to the cosmological expansion, for relatively nearby galaxies outside the local group, distance is proportional to redshift to a good approximation, with
$r = cz/H_0$,
where $c$ denotes the speed of light, and $H_0$ is Hubble's constant, measuring the current expansion rate of the universe, with $H_0 \approx 70$~km~s$^{-1}$~Mpc$^{-1}$ (with Mpc denoting megaparsecs).
$H_0$ is measured with a precision of several percent, and spectroscopic redshifts for nearby galaxies can be measured to sub-percent precision.
For simplicity, here we consider distances to be precisely measured, via spectroscopic redshifts.
Often, astronomers use redshift directly as a proxy for distance.

As noted previously, a fundamental intrinsic characteristic of a source is its luminosity, $L$, a measure of its intrinsic (vs.\ apparent) brightness, related to flux and distance via the inverse-square law.
The \emph{luminosity function}, $\lfunc(L, r)$, describes the distribution of luminosities for a population at a specified distance (or redshift).
It is typically defined as the intensity function for a point process, i.e., as specifying the expected number of galaxies per unit volume at distance $r$, per unit luminosity interval.
If we denote the spatial number density of galaxies at distance $r$ by $n(r)$ (galaxies per unit volume), then $n(r) = \int dL\, \lfunc(L, r)$.
The \emph{luminosity PDF} for galaxies at distance $r$ is then
\begin{equation}
\lpdf(L,r) = \frac{\lfunc(L, r)}{n(r)}.
\label{eq:lpdf}
\end{equation}
Note that $\lfunc(L,r)$ and $\lpdf(L,r)$ specify \emph{conditional} distributions, i.e., distributions for $L$ at a given $r$.
(Authors vary on the definition of the luminosity function, many defining it as done here, and others using ``luminosity function'' to denote what we here call the luminosity PDF.)
As noted above, the \emph{flux PDF} for galaxies at distance $r$, denoted $\rho(F,r)$, can be found by a change of variables from $L$ to $F$, giving
\begin{equation}\label{eq:fluxPDF}
\rho(F,r) = 4\pi r^2 f(4\pi r^2 F, r).
\end{equation}

% \noindent\enote{Define the distance PDF, $h(r)$, here?  Or leave it below?}

The galaxy luminosity function carries valuable information about the formation and evolution of galaxies, therefore it is an important target of inquiry in astronomy (see \citealt{BST88-LumFuncReview} and \citealt{J11-LumFuncReview} for reviews).
\cite{J11-LumFuncReview} provides a review of methods developed by astronomers for estimation of galaxy luminosity functions.
A nontrivial challenge is careful accounting of both measurement error and selection effects, which are related for most survey datasets.
This section outlines how these complications can be modeled (with details provided in Appendix~A).
Fundamentally, the model does not have the simple single-plate DAG structure that motivated the design of CUDAHM.
However, the joint posterior may be manipulated into a form well-suited to CUDAHM.
Luminosity function inference thus provides an example of the flexibility of the framework.
The next section presents a simulation study demonstrating CUDAHM luminosity function inference for simulated populations with sizes up to $10^6$.

% For simplicity we here focus on a homogeneous population, so that $n(r)$ is considered fixed (constant with respect to volume measure).
% The following development is straightforward to generalize to account for models with distance dependence in the luminosity function.


%-------------------------------------------------------------------------------
\subsection{Modeling survey selection effects and measurement error}
\label{sec:slxn+err}

Astronomers estimate luminosity functions and other astronomical distributions using data compiled in \emph{survey catalogs}: tables of estimates (including uncertainties) of object properties, accompanied by a description of selection criteria for the survey that produced the catalog.
Catalogs are derived data products; they summarize information in more complex and voluminous raw datasets, such as large collections of images or time series.
The nature of astronomical catalog data makes their analysis differ in some respects from analyses of survey data familiar in the statistical survey sampling literature.

Flux measurements are affected by measurement error that is often dominated by Poisson fluctuations in the photon counting rate, including fluctuations from astrophysical and instrumental backgrounds.
The measurement error thus approximately scales with the square root of the flux, and is fractionally greater at low flux than at high flux.
At low fluxes, photons from the backgrounds can produce false source detections.
To prevent this, surveys adopt detection criteria to strongly mitigate against false detections.
A simple, representative criterion is to accept sources only if the estimated flux is $\nu$ times the flux uncertainty, with $\nu \approx 5$ so that the probability for false detection is low even for large catalogs (i.e., there is strong control of the family-wise error rate).
Here we assume we are in the pure catalog regime, i.e., that the threshold is set high enough that the resulting catalog may be considered to have no false detections.
(The framework described below is straightforward to generalize to settings where false detections may be present, but many catalogs adopt high thresholds so we focus on the simpler pure catalog regime here.)

Detection criteria introduce \emph{selection effects} into catalogs.
Most obviously, faint sources (low luminosity sources, or distant high luminosity sources) are excluded; the observable luminosity function is a thinned version of the actual luminosity function.
In addition, measurement error more subtly but significantly distorts the shape of the observable distribution, a phenomenon well known in the density deconvolution literature, and also recognized in the astronomical literature, where it is sometimes called \emph{Eddington bias}, in reference to early discussions of the distortion by Eddington and Jeffreys (\citealt{J38-EddBias,E40-EddBias}).
They noted that an object with a measured flux of $\hat F$ is more likely to be an object with a true flux $F < \hat F$ than one with $F > \hat F$, even when measurement errors have a symmetric distribution, because dim sources are more numerous than bright sources in most astronomical settings.
Selection effects can exacerbate the distortion in the vicinity of a flux threshold, with measurement error and the falling flux distribution conspiring to scatter more below-threshold sources into the observed sample than above-threshold sources out of it, a component of what astronomers call \emph{Malmquist bias} (\citealt{LH10-BMIC}; but note that the term is used inconsistently in the literature).
Hierarchical modeling can automatically account for such thinning and distortion, in a manner that adapts to the shape of the luminosity function; this is a major motivation for its increasing popularity in astrostatistics.

We have developed a framework for modeling astronomical survey data using \emph{thinned latent marked point process models}.
Measurement error is handled in a hierarchical Bayesian manner, by introducing latent member property parameters, with catalog estimates understood as describing member likelihood functions for the properties.
We model the population distribution of the latent member properties using marked point processes.
We model selection effects through thinning of the latent point process.
This framework was originally developed for studying the luminosity function of gamma-ray bursts, powerful explosive transients thought to mark the birth of stellar-mass black holes (\citealt{LW95-GRBs-TLPP,LW98-GRBs-Iso}).
It was subsequently adapted to study the luminosity distribution (and through it, the size distribution) of trans-Neptunian objects, asteroid-like minor planets in the outer solar system (\citealt{L04-MsmtErr,P+08-TNOSizeDistn}).
We outline the framework here as it applies to luminosity function estimation and similar problems, in contexts where a marked Poisson point process is an appropriate model for the latent member properties.
\cite{kelly2008flexible} independently developed a similar approach for settings where a binomial point process may be appropriate, and applied it to estimating the number density of quasars as a function of redshift.

A somewhat subtle aspect of the problem is the tie between measurement error and selection effects.
In astronomical surveys, the raw data are searched to find candidate objects.
For candidates that pass detection criteria, the data are used to estimate source properties.
The same underlying data are used both for selection (detection) and measurement; these tasks are not independent, as is assumed in many statistical survey methods outside of astronomy.
Ignoring the dependence can corrupt inferences.

%Modeling selection effects requires considering the properties of objects that were excluded from the catalog, i.e., objects that were present but that produced data that did not meet detection criteria.

In Appendix~A, we derive the likelihood functions for the parameters of a luminosity function or luminosity PDF, based on catalog data from a survey subject to measurement error and selection effects in fluxes, and reporting precise distance measurements, $r_i$.
Denoting the luminosity PDF parameters by $\rhopar$, the likelihood function takes the form
\begin{equation}\label{eq:rho-mlike}
\like(\rhopar)
  = \prod_{i=1}^N \int dF\, \epdf(F,r_i;\rhopar)\, \ell_i(F),
\end{equation}
where we have introduced an \emph{effective density} for the latent observables, $F$ and $r$, related to the luminosity PDF, $\rho(F,r;\rhopar)$, according to
\begin{equation}\label{eq:epdf}
\epdf(F,r;\rhopar) \equiv
  \frac{h(r) \rho(F,r;\rhopar)}
    {\int dr \int dF\,\aeffic(F)\, h(r)\, \rho(F,r;\rhopar)},
\end{equation}
where $h(r)$ is the distance distribution (assumed uniform in volume, and thus $\propto r^2$ as a function of distance; see Appendix~A), and $\aeffic(F)$ is the average detection efficiency for sources with flux $F$ (the detection efficiency at each location in the images comprising the survey, averaged over location).
Equation~(\ref{eq:rho-mlike}) resembles the likelihood function for a binomial point process, but with observations that have measurement errors described by the member likelihood functions.
However, the analogy is not exact, because the effective density is not a PDF; it is renormalized by the fraction of the population detectable by the survey (the numerator in \eqref{eq:epdf}).

Although this model does not exactly match the simpler model structures discussed previously, the structure of the likelihood function in (\ref{eq:rho-mlike}) is essentially the same as that for conditional density estimation with measurement error (the middle DAG in Fig.~\ref{fig:DAGs}).
It has a single product term, composed of independent factors for each detected object---just the type of structure CUDAHM was designed to sample from.